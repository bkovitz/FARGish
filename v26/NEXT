Soup

    NEXT Implement reconstruction of 'ajaqb' using only 2-cell Segs.

    Implement a painter with 'unification'.

RMem

    A LoggedStep class to track what happened during a run. DONE

    Rig up an exp() function, or an atest, to verify that an RMem
    trained on only a single equation can regenerate that equation reliably.
    DONE

    To absorb a new equation, immerse it in a canvas with 'related' equations:
    equations that randomly selected parts of the new equation evoke.
    DONE (no atest)

    Absorb many equations via immersion with related equations. See if it
    becomes more or less accurate as the number of equations increases.
    WOW, it's 100% accurate in 50 timesteps.  DONE

    Make a relative painter:  match 1, paint '+' in next space to the
    right.

        RMem.as_addr_from(c, from)  DONE

        Run it in a unit test on 1+1=2.  DONE

    Remove methods in RMem that are now obsolete because of
    as_absolute_painters().

    Refactor RMem so it contains just the minimal interface. DONE

    Try a more Hopfieldesque run: all other cells get a vote each time a
    cell is updated. rndfunc cells give fractional votes. DONE

        Make a subclass of Regenerate that overrides .regenerate(). DONE

        Rig up an easy way to specify and run a single test of
        WithAllRunnablePainters, showing all the values and weights at
        each timestep.

    Rig up a test that recalls only painters.

    Make an automated test that runs through several standard equation sets,
    including just 1+1=2, just a few, just addition with all grade-school
    operands, and the whole grade-school table; and parameterize this test so
    it can run all sorts of variants on how to make and run painters.
    Report results on how many equations were recalled correctly, and
    how well it did with equations that it wasn't trained on.

    On paper, see what happens in 1+1=2 with relative painters in the LTS
    and absolute painters during reconstruction.

    On paper, do 1+1=2 Jumbo-style.  DONE?

    Relative painters that paint absolute painters. Why did this seem
    like a good idea?

        Make run_painter() make a relative painter paint an absolute
        painter. DONE?

            Write .is_abs_painter()  DONE

            Paint an absolute painter from a relative painter.
            make_abs_painter().  Maybe this is already done, implemented
            by .as_absolute_painters(). So, write a unit test for it. DONE

        Sketch out the code for this. To go in WithRelsPaintAbsolutes.

    Watch the lsteps in some runs with correction_redundancy.

    Rig up an xp where the second-order painters reconstruct the first-order
    generators.

    The current problem is that 'same' across dups in a canvas tends
    to lock in a bad move. Solution to try: Wider canvas, so that 2 + 3 = 5
    becomes:

               2 * 3 = 6
               2 + 4 = 5
               2 + 3 = 5   <-- central canvas
               2 + 2 = 4
               1 + 3 = 4
               3 + 3 = 6


    rndfunc: Put conditions on functions, like "if these cells are the same".

    as_canvas: enables overriding the Canvas class. Include the Canvas class
    as a field of RMem. Make one version that bumps clarity in discrete
    steps, and another that increases clarity linearly and decreases it
    exponentially.

    Let painters adjust the size of the canvas.


    Put painters in the canvas. This might implement 'relative addressing' and
    'treat 1 as 10'.

    raw_make_painters(): All relative addressing (to the right of the 1,
    paint a '+'), but distinguish whenever there's a conflict (by making
    painters that require two anchor points). These relative painters
    paint absolute painters.

        Later, distinguish in the same way when absorbing painter-sets into
        the long-term memory.

ON PAPER

    Make a minimal list of cognitive biases needed for Numbo.

    Figure out how to get them from regenerative memory.

IDEAS

    Tags can be stored with absolute addresses.

    Annotations are values stored in additional cells--tags!

    Some "painters" can be taggers, which test for a condition.

    Running a conditional painter that needs a tag causes the tagger to be
    created or activated.

    An ambiguity: +1 on a generator: does it increment addr1, addr2, or the
    func?

    The main "substitution" should be on addresses, not values. ?

    An addr is a sort of function: it "returns" the contents of its cell.


    Instead of func_from_to, how about a function that maps a generator
    to a generator, i.e. including relations between the addresses?

    What would be really good is if painters could generate the painters
    themselves (replacing make_generators).

    Painting a value might superimpose determine values over Nones, where
    each value includes a generator:
                  (None, None) None  '+'
       paint:     (prev, next) same  None
       result:    (prev, next) same  '+'


    To absorb a new canvas: run the existing memory on it (or parts of it),
    and add painters to correct errors.

    A step toward "painters in canvases": make the passive cells active,
    so the + in '1 + 1 = 2' paints (prev, next, same).

    How does the model creatively redefine '+'?

    How does the model chunk?

    How does the model handle Consume(xs)?

    I'm thinking: via recursion, via painters that construct painters.

    The model should evolve the way the acclivation models evolved: by
    exposure to varied stimuli.

    The "clearer" a painter in the soup, the more likely to run.

    To "average" two conflicting painters, incorporate a relation with a third
    cell. Maybe need an entire "shadow" set of painters, one for every cell.

        avg(p1, p2) = two painters, one in the long-term soup, one in the
        shadow canvas. Or perhaps many painters in the LTS, which produce
        one in the shadow canvas. This enables relations with other cells
        to contribute.

    Instead of constant painters, paint to the shadow canvas. In 1+1=2,
    (1, 0, paintfrom(shadow(to))) shadow(0) = paint(1)
    (1, 0, painterfrom(shadow(to))), shadow(0) = paint(1)
    Wait, how do we get the + involved? (+, before(+), 1)
    (1, shadow(0), paint(1))
    (4, shadow(0), paint(-1))   # indirection  @4 = 2; -1(@4) = 1
    Relative overwrites absolute.
    That changes absolute to relative. Now how do we get pressure for x
    against +?

        To average 1 and x2:

    Could we paint to an edge? Isn't that what painting a painter means?
    When we run or make a canvas, do we need a cell for every edge? Would
    that lead to infinite regress? Perhaps the shadow cells' addresses
    could be edges.

        Maybe the LTS is nothing but painters with addresses of the form
        (cell, edge).

    In Robinsonizing, global properties (value counts) flow into cells
    and cell values contribute to global properties.

    The arguments to a painter should always include "my address" as well as
    the current contents of that address.

    Let the arguments to every func be:  from, to, painter.  Every value
    should also be a painter.

        So then how does +1 work? +1 means "However many times f does its
        thing, return a painter that does it once more."

    Could each cell be a little canvas, with three cells of its own: from, to,
    painter? This means that each cell is also an edge. But why not infinite
    regress?

        In computers, address, data, and code are all the same. But the
        interpretation depends on what's reading it, i.e. what part of the
        'paint next value' machinery is reading it. What addr is '+'?
        What should be stored in the soup?

    If a painter refers to another cell, that increases the 'demand' for that
    cell.

    Incorporate value frequencies as in cryptograms?

    "Absorbing" something familiar might turn a knob that increases its
    probability of being redrawn, as opposed to modifying the graph of
    cascades.

    START and END are really parentheses--which can be nested. These can
    enclose multi-digit numbers as well as subexpressions, and they are
    also text. Could parentheses indicate 'types', like tags? This would
    enable relative addressing within numbers to be distinct from within
    subexpressions. Nah, better to go by what follows a parenthesis.

    How do we "insert" text before or after something without falling easily
    into infinite loops (generating infinitely long strings)?

        Maybe insert by referring relatively. If you refer to something
        before you, that implicitly creates a cell. If the cell doesn't get
        filled in pretty soon, though, it disappears.

    We might get something like currying by limiting the number of arguments
    to painters.
    
    Make the '+' of Hopfield into a func that works with the global canvas to
    recreate either operand. Does this make Robinsonizing into a special case
    of regenerative memory? Would there need to be a function analogous to
    sgn, or could the "averaged" function just paint and be overwritten?

        A global property: a waveform representation, or number-line
        representation.

    Could the sequence of paintings itself be painted based on history of
    painting sequences?

    An ordering operator? Indicates where to put canvas-chunks in relation to
    each other?

    Really, painters should coalesce around a "north star".

    Does make_generators() have a fixed point? If the number of generators
    stops growing at some point, this would make it a lot like Robinsonizing.

    Don't just paint from one origin cell, paint from several.

    Make each painter work by relative addressing. This is the simplest form
    of orientation. Favor relative addresses as well as relative values.

    Orientation cues might be relative, too. Painting the + "between" the 1's
    should be applicable with other numbers serving as "1's".

    One painter might call others.

    Have a small library of painters. Try to construct painters by
    parameterizing or extending existing painters.

    Let the relative, multi-origin painters evolve from the one-origin
    painters by resolving conflicts. We should start absolute and go relative
    under pressure. The secret is that the system responds easily to
    "go relative" and can go relative any number of levels. We make
    error-correctors, and then correctors for the error-correctors, and
    so on.

        This suggests that the correction-redundancy mechanism should
        introduce relative painters, or painter-painters to correct the
        errors found in the recalled canvases that don't match the present
        canvas.

    Are relations between relations something like Taylor's formula, where
    all the derivatives of f can reconstruct f (under favorable conditions)?

    The main problem right now: how to paint the operators. They have no
    relation with anything else, hence have high information content, hence
    are unpredictable, hence get arbitrarily redrawn a lot. But really,
    there is a relation between the operators and the numbers. That's the
    point of the equations! By evolution of cascades, the net should
    figure out that relation.

    Reference points can only emerge from "averaging".

    1 _ 1 -> 1 + 1
    same _ same -> same + same   Derive relative painters from absolute
                                 whenever possible, and reconstruct the
                                 absolute when regenerating.

                                 Exercise: make a "Taylor series" for
                                 1+1=2.

    Three-cell relative painters are little absolute psets? To regenerate,
    move the three-cell window along?

        We always need to be able to regenerate the whole canvas from one
        cell.

        Wherever there is a conflict, we need disambiguation. A random
        painter with two equally probable alternatives needs additional
        information to settle the choice. A rare *start condition* (requires
        a relative addr) is distinctive, therefore should have a lot of
        force in triggering follow-on painters.

    A relation-only RMem can't get started from a single absolute value.
    So maybe include the first value in some painter.

    Exponential decay: after being written to, painters with an interest in a
    cell should check it. This is the opposite of "gatekeepers". Both
    hypotheses should be run. Exponential decay doesn't solve the problem of
    rndfuncs painting wrong values; that's from exceeding the memory capacity.
    But it might get the system to reach attractors sooner.

    Could a subcanvas be not a separate object but simply a part of a plain
    canvas designated in some way, e.g. the 'before' part? Then painters
    could designate it as well as use it.

    Match on a group, not just a value.

    Annotate each cell with the painter who painted its value?

    Try putting '1+1=2' as the salt for all equations. Or simply
    '1234567890+-x/'. Then every constant can be painted as a relation by
    at least one other cell.

    There needs to be a "mutational" mechanism: a way to generate variations
    on canvas values, painters, addresses, relations, functions. Thus the
    model can explore "nearby" possibilities, turn "knobs", and alter the
    "graph" of cascades. And it should be possible to create knobs via those
    cascades, as in the acclivation paper. With virtual knobs, it should be
    possible to alter the metric of "near" variations and to make some parts
    of the space inaccessible.

    If there is a mutational mechanism, there must also be a mechanism to
    preserve the ones that work best--not only the mutations that solve
    problems, but the mutational mechanisms (or cascades) that find those
    solutions the fastest.

    "Canvas name" might be a nice additional thing for a relative painter
    to match on (as well as jump to). More broadly, we might consider any
    sort of annotation on a cell. But the best annotation, surely, for
    indirection, is "the painter who painted this".

    Matchers with 'don't care' masks, as in CAM. Or various forms of 'lowering
    the standards' of a match. These can be explored by the mutation generator.

    What we really want is for the painters in the long-term soup to become
    more likely to run on 'from' and 'to' addresses in a suitable context.
    Two painters ('1', right1, '+') and ('1', right1, '=') should each tend
    to run only in an appropriate context: at the left side of an expression
    and at the right side of an expression, respectively. But, lacking such
    context, these painters also need the ability to help get such a context
    created.

    If we think of a matcher for '1' as meaning "the" 1, then it needs a
    context in which there is only a single 1.

    Layers of attractor basins. "Averaging" two relative painters should
    create a basin in the potential space that contains twio smaller basins.
    Other factors should determine which of the smaller basins the system
    goes to.

    An axiom: Everything non-local must become local in order to have an
    effect. On the other hand, this is violated by absolute painters.

    When we paint a '1' in a cell, we could actually paint (1, right1, +),
    which 'looks like' 1 when viewed. Then every cell value would also be
    a painter. Would this gain anything?

    "Look where the information is."

    We can install an attractor in a Hopfield net. How about installing two:w

    If a relative painter can't find its cue but itself has a high clarity or
    activation level, maybe then it should paint its cue. Even more so if the
    other things it paints are already there.

    To average painters, blur the result? Then need a way for multiple
    blurry paintings to "sharpen".

    Is there any value in asking, "Who sees this?" That is, which painters
    respond to it?

    A principal goal of spreading activation: to recreate the context in
    which something occurred previously, but built from elements in the
    current context. Words need to evoke contexts in which they occur;
    numbers need to evoke equations in which they occur; equations need
    to evoke deductions from other equations and nearby or landmark variants.

    How can we break apart more of the process into little bits that
    coalesce? The building of functions, the building of painters,
    querying the soup?

    Some notion of expectation should prime searches at every level.
    Should everything have a before/now pair?

    Is it OK for a "self-description" canvas to have a column indicating only
    the presence of a certain type of painter? More pertinently: what
    columns should there be for painters, or descriptions of painters?

    Should each painter paint only the next iteration of the canvas? Well,
    that's what painters do, no? But it seems that we have both canvas and
    soup. The soup persists and accumulates through many iterations of the
    canvas.

IDEAS FOR HOW TO CONVERGE ON AN AUTOCATALYTIC SET OF PAINTERS

    There is one process that makes painters from what's on the canvas, and
    maybe from the current set of painters. There is another process, that
    paints the canvas, and maybe painters. When process A constructs the
    painters of process B, and process B paints the canvas cells of A, then we
    have an autocatalytic set.

    Now, what condition makes it likely or necessary that running these two
    processes simultaneously will lead to an autocatalytic set?

    Painters, possibly from the long-term memory, might perform process A.
    Painters in the long-term memory don't need to be part of the autocatalytic
    set, even though they're necessary to run the process.



NUMBO

    Not to be missed: a big part of solving numbles is thinking of ways to
    define small subproblems that can be exhaustively checked. The same
    thing happens in Wordle: you think "how many words fit S_ITS" and
    compare that to how many moves you have left. We define a little "space"
    to search--the space consisting of the remaining blanks, holding
    current guesses constant.

    Backup plan: Make a lot of codelets manually but still operate on
    canvases. E.g. a codelet to check and correct arithmetic, codelets that
    operate on little canvases that explore ideas or try to anticipate
    where another canvas is headed.

    A FARGish spec could simply be a set of basis functions/painters, a set
    of symbols, a canvas class, and rules for pressures.

    The role of "averaging" is to make a painter that will paint different
    values depending on what values other painters support. That is, the
    average painter should cooperate with other painters to paint a value
    they both agree on. So, perhaps an average painter should paint a painter
    that somehow decides what to paint based on some other painter.

NOTES

    The codelets are the memory.

    Just as our bodies are colonies of 40 trillion microorganisms, our minds
    might be colonies of 40 trillion codelets.

    We don't start from random weights, we start from a blank slate.
    And we don't need millions of trials.

    For years, I came up with schemes to explain flexibility, but they always
    had peculiar limitations that people lack. Even spreading-activation
    networks couldn't enable everything to interact reasonably with everything
    else. A major obstacle was a limitation on levels, such as applying
    numbers to the numbers' positions within a sequence. The solution was
    stigmergy: put everything on one level, including the painters that
    draw on the canvas.

    How is the Hopfield net, or RMem, *indexed*? You look stuff up in a card
    catalog by author or title; your brain indexes words by starting letter.
    What indexing do you get from painters that can start anywhere?

    The Hopfield net as a model of computation--as a model of goal-directed
    computation. Some painters may need to be insistent about getting their
    way (via "testers")? We demand certain relationships between parts of
    the canvas and keep pushing the search until we get them.

    Principle: reconstruct the context in which each element was seen--and
    the context in which that context was seen, and so on.

    12-Feb-2022: My major concern right now: how to make an iterative
    process that converges on painters that converge on the canvas.

    The lambda calculus has nothing like "addressing"--unless, of course,
    you make an interpreter in the lambda calculus for a programming
    language that does have addressing. But in FARGish we don't want to
    go out so many levels. We want interesting things to be constructed
    pretty simply. And yet, shouldn't a FARGish model have the ability to
    rig up its own "interpreter"--say, for arithmetic?

    Regarding a Hopfield net that can reconstruct its own edge weights:
    could this be done with relative painters? "Next to these edge weights,
    put this edge weight." But then how do we reconstruct the relative
    painters? Do relative painters not have the problem of exponentially
    growing addresses?

    Could a 'context' marker (e.g. "true equation", "mistake to avoid")
    function like an absolute address in SDM, i.e. allowing for the marker
    to be 'near' the marked items?

LESSONS LEARNED FROM FARG MODELS

    Slippage: learned to see it everywhere, even within a musical melody, or
    a melody slipping to accommodate the words of a song. Learned to see
    how slippage is ubiquitous at all levels, not a special last resort.

        More examples: erroneous subject-verb agreement by proximity.

HUMAN-SUBJECT STUDIES

    A test of serial-position effect: Give people a word list, and to probe
    them, tell them XXX was in the list, and then ask if YYY was in the list.
    Will it help if YYY came just after XXX? What will happen if XXX was not
    really in the list?

MORE TESTS OF THE MODEL

    Will an RMem show the serial-position effect? Train it on a large
    vocabulary of English words, and then train it on a list, and ask if a
    given word was in the list.

    Treisman's false-conjunction effect? Represent features as generators (or
    little clusters of generators). The features are unordered. Let them paint
    a canvas and see how they position themselves.

    Regenerating a temporal sequence: the canvas is what happened recently,
    a None for what happens next, and maybe some cells for what the system
    expects to do soon. Let the system fill the 'next' cell, and then shift
    the canvas one cell to the left.
