RMem

    A LoggedStep class to track what happened during a run. DONE

    Rig up an exp() function, or an atest, to verify that an RMem
    trained on only a single equation can regenerate that equation reliably.
    DONE

    To absorb a new equation, immerse it in a canvas with 'related' equations:
    equations that randomly selected parts of the new equation evoke.
    DONE (no atest)

    Absorb many equations via immersion with related equations. See if it
    becomes more or less accurate as the number of equations increases.
    WOW, it's 100% accurate in 50 timesteps.  DONE

    Make a relative painter:  match 1, paint '+' in next space to the
    right.

        RMem.as_addr_from(c, from)  DONE

        Run it in a unit test on 1+1=2.  DONE

    Remove methods in RMem that are now obsolete because of
    as_absolute_painters().

    Refactor RMem so it contains just the minimal interface. DONE

    Try a more Hopfieldesque run: all other cells get a vote each time a
    cell is updated. rndfunc cells give fractional votes.

        NEXT Make a subclass of Regenerate that overrides .regenerate().

    Rig up a test that recalls only painters.

    Make an automated test that runs through several standard equation sets,
    including just 1+1=2, just a few, just addition with all grade-school
    operands, and the whole grade-school table; and parameterize this test so
    it can run all sorts of variants on how to make and run painters.
    Report results on how many equations were recalled correctly, and
    how well it did with equations that it wasn't trained on.

    Watch the lsteps in some runs with correction_redundancy.

    Rig up an xp where the second-order painters reconstruct the first-order
    generators.

    The current problem is that 'same' across dups in a canvas tends
    to lock in a bad move. Solution to try: Wider canvas, so that 2 + 3 = 5
    becomes:

               2 * 3 = 6
               2 + 4 = 5
               2 + 3 = 5   <-- central canvas
               2 + 2 = 4
               1 + 3 = 4
               3 + 3 = 6


    rndfunc: Put conditions on functions, like "if these cells are the same".

    as_canvas: enables overriding the Canvas class. Include the Canvas class
    as a field of RMem. Make one version that bumps clarity in discrete
    steps, and another that increases clarity linearly and decreases it
    exponentially.

    Let painters adjust the size of the canvas.


    Put painters in the canvas. This might implement 'relative addressing' and
    'treat 1 as 10'.

    raw_make_painters(): All relative addressing (to the right of the 1,
    paint a '+'), but distinguish whenever there's a conflict (by making
    painters that require two anchor points). These relative painters
    paint absolute painters.

        Later, distinguish in the same way when absorbing painter-sets into
        the long-term memory.

IDEAS

    Tags can be stored with absolute addresses.

    Annotations are values stored in additional cells--tags!

    Some "painters" can be taggers, which test for a condition.

    Running a conditional painter that needs a tag causes the tagger to be
    created or activated.

    An ambiguity: +1 on a generator: does it increment addr1, addr2, or the
    func?

    The main "substitution" should be on addresses, not values. ?

    An addr is a sort of function: it "returns" the contents of its cell.


    Instead of func_from_to, how about a function that maps a generator
    to a generator, i.e. including relations between the addresses?

    What would be really good is if painters could generate the painters
    themselves (replacing make_generators).

    Painting a value might superimpose determine values over Nones, where
    each value includes a generator:
                  (None, None) None  '+'
       paint:     (prev, next) same  None
       result:    (prev, next) same  '+'


    To absorb a new canvas: run the existing memory on it (or parts of it),
    and add painters to correct errors.

    A step toward "painters in canvases": make the passive cells active,
    so the + in '1 + 1 = 2' paints (prev, next, same).

    How does the model creatively redefine '+'?

    How does the model chunk?

    How does the model handle Consume(xs)?

    I'm thinking: via recursion, via painters that construct painters.

    The model should evolve the way the acclivation models evolved: by
    exposure to varied stimuli.

    The "clearer" a painter in the soup, the more likely to run.

    To "average" two conflicting painters, incorporate a relation with a third
    cell. Maybe need an entire "shadow" set of painters, one for every cell.

        avg(p1, p2) = two painters, one in the long-term soup, one in the
        shadow canvas. Or perhaps many painters in the LTS, which produce
        one in the shadow canvas. This enables relations with other cells
        to contribute.

    Instead of constant painters, paint to the shadow canvas. In 1+1=2,
    (1, 0, paintfrom(shadow(to))) shadow(0) = paint(1)
    (1, 0, painterfrom(shadow(to))), shadow(0) = paint(1)
    Wait, how do we get the + involved? (+, before(+), 1)
    (1, shadow(0), paint(1))
    (4, shadow(0), paint(-1))   # indirection  @4 = 2; -1(@4) = 1
    Relative overwrites absolute.
    That changes absolute to relative. Now how do we get pressure for x
    against +?

        To average 1 and x2:

    Could we paint to an edge? Isn't that what painting a painter means?
    When we run or make a canvas, do we need a cell for every edge? Would
    that lead to infinite regress? Perhaps the shadow cells' addresses
    could be edges.

        Maybe the LTS is nothing but painters with addresses of the form
        (cell, edge).

    In Robinsonizing, global properties (value counts) flow into cells
    and cell values contribute to global properties.

    The arguments to a painter should always include "my address" as well as
    the current contents of that address.

    Let the arguments to every func be:  from, to, painter.  Every value
    should also be a painter.

        So then how does +1 work? +1 means "However many times f does its
        thing, return a painter that does it once more."

    Could each cell be a little canvas, with three cells of its own: from, to,
    painter? This means that each cell is also an edge. But why not infinite
    regress?

        In computers, address, data, and code are all the same. But the
        interpretation depends on what's reading it, i.e. what part of the
        'paint next value' machinery is reading it. What addr is '+'?
        What should be stored in the soup?

    If a painter refers to another cell, that increases the 'demand' for that
    cell.

    Incorporate value frequencies as in cryptograms?

    "Absorbing" something familiar might turn a knob that increases its
    probability of being redrawn, as opposed to modifying the graph of
    cascades.

    START and END are really parentheses--which can be nested. These can
    enclose multi-digit numbers as well as subexpressions, and they are
    also text. Could parentheses indicate 'types', like tags? This would
    enable relative addressing within numbers to be distinct from within
    subexpressions. Nah, better to go by what follows a parenthesis.

    How do we "insert" text before or after something without falling easily
    into infinite loops (generating infinitely long strings)?

        Maybe insert by referring relatively. If you refer to something
        before you, that implicitly creates a cell. If the cell doesn't get
        filled in pretty soon, though, it disappears.

    We might get something like currying by limiting the number of arguments
    to painters.
    
    Make the '+' of Hopfield into a func that works with the global canvas to
    recreate either operand. Does this make Robinsonizing into a special case
    of regenerative memory? Would there need to be a function analogous to
    sgn, or could the "averaged" function just paint and be overwritten?

        A global property: a waveform representation, or number-line
        representation.

    Could the sequence of paintings itself be painted based on history of
    painting sequences?

    An ordering operator? Indicates where to put canvas-chunks in relation to
    each other?

    Really, painters should coalesce around a "north star".

    Does make_generators() have a fixed point? If the number of generators
    stops growing at some point, this would make it a lot like Robinsonizing.

    Don't just paint from one origin cell, paint from several.

    Make each painter work by relative addressing. This is the simplest form
    of orientation. Favor relative addresses as well as relative values.

    Orientation cues might be relative, too. Painting the + "between" the 1's
    should be applicable with other numbers serving as "1's".

    One painter might call others.

    Have a small library of painters. Try to construct painters by
    parameterizing or extending existing painters.

    Let the relative, multi-origin painters evolve from the one-origin
    painters by resolving conflicts. We should start absolute and go relative
    under pressure. The secret is that the system responds easily to
    "go relative" and can go relative any number of levels. We make
    error-correctors, and then correctors for the error-correctors, and
    so on.

        This suggests that the correction-redundancy mechanism should
        introduce relative painters, or painter-painters to correct the
        errors found in the recalled canvases that don't match the present
        canvas.

    Are relations between relations something like Taylor's formula, where
    all the derivatives of f can reconstruct f (under favorable conditions)?

    The main problem right now: how to paint the operators. They have no
    relation with anything else, hence have high information content, hence
    are unpredictable, hence get arbitrarily redrawn a lot. But really,
    there is a relation between the operators and the numbers. That's the
    point of the equations! By evolution of cascades, the net should
    figure out that relation.

    Reference points can only emerge from "averaging".

    1 _ 1 -> 1 + 1
    same _ same -> same + same   Derive relative painters from absolute
                                 whenever possible, and reconstruct the
                                 absolute when regenerating.

                                 Exercise: make a "Taylor series" for
                                 1+1=2.

    Three-cell relative painters are little absolute psets? To regenerate,
    move the three-cell window along?

        We always need to be able to regenerate the whole canvas from one
        cell.

        Wherever there is a conflict, we need disambiguation. A random
        painter with two equally probable alternatives needs additional
        information to settle the choice. A rare *start condition* (requires
        a relative addr) is distinctive, therefore should have a lot of
        force in triggering follow-on painters.

    A relation-only RMem can't get started from a single absolute value.
    So maybe include the first value in some painter.

    Exponential decay: after being written to, painters with an interest in a
    cell should check it. This is the opposite of "gatekeepers". Both
    hypotheses should be run. Exponential decay doesn't solve the problem of
    rndfuncs painting wrong values; that's from exceeding the memory capacity.
    But it might get the system to reach attractors sooner.

    Could a subcanvas be not a separate object but simply a part of a plain
    canvas designated in some way, e.g. the 'before' part? Then painters
    could designate it as well as use it.

NUMBO

    Not to be missed: a big part of solving numbles is thinking of ways to
    define small subproblems that can be exhaustively checked. The same
    thing happens in Wordle: you think "how many words fit S_ITS" and
    compare that to how many moves you have left. We define a little "space"
    to search--the space consisting of the remaining blanks, holding
    current guesses constant.

NOTES

    The codelets are the memory.

    Just as our bodies are colonies of 40 trillion microorganisms, our minds
    might be colonies of 40 trillion codelets.

    We don't start from random weights, we start from a blank slate.
    And we don't need millions of trials.

    For years, I came up with schemes to explain flexibility, but they always
    had peculiar limitations that people lack. Even spreading-activation
    networks couldn't enable everything to interact reasonably with everything
    else. A major obstacle was a limitation on levels, such as applying
    numbers to the numbers' positions within a sequence. The solution was
    stigmergy: put everything on one level, including the painters that
    draw on the canvas.

    How is the Hopfield net, or RMem, *indexed*? You look stuff up in a card
    catalog by author or title; your brain indexes words by starting letter.
    What indexing do you get from painters that can start anywhere?

    The Hopfield net as a model of computation--as a model of goal-directed
    computation. Some painters may need to be insistent about getting their
    way (via "testers")? We demand certain relationships between parts of
    the canvas and keep pushing the search until we get them.


LESSONS LEARNED FROM FARG MODELS

    Slippage: learned to see it everywhere, even within a musical melody, or
    a melody slipping to accommodate the words of a song. Learned to see
    how slippage is ubiquitous at all levels, not a special last resort.

        More examples: erroneous subject-verb agreement by proximity.

HUMAN-SUBJECT STUDIES

    A test of serial-position effect: Give people a word list, and to probe
    them, tell them XXX was in the list, and then ask if YYY was in the list.
    Will it help if YYY came just after XXX? What will happen if XXX was not
    really in the list?

MORE TESTS OF THE MODEL

    Will an RMem show the serial-position effect? Train it on a large
    vocabulary of English words, and then train it on a list, and ask if a
    given word was in the list.

    Treisman's false-conjunction effect? Represent features as generators (or
    little clusters of generators). The features are unordered. Let them paint
    a canvas and see how they position themselves.

    Regenerating a temporal sequence: the canvas is what happened recently,
    a None for what happens next, and maybe some cells for what the system
    expects to do soon. Let the system fill the 'next' cell, and then shift
    the canvas one cell to the left.
